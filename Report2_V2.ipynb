{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0899e06c",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e121e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc  # Para coleta de lixo explícita\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53173f10",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = pd.read_csv(\"product_structures_sales.csv\")\n",
    "prices_df = pd.read_csv(\"product_prices_leaflets.csv\")\n",
    "campaigns_df = pd.read_csv(\"chain_campaigns.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a226a48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>structure_level_4</th>\n",
       "      <th>structure_level_3</th>\n",
       "      <th>structure_level_2</th>\n",
       "      <th>structure_level_1</th>\n",
       "      <th>sku</th>\n",
       "      <th>time_key</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3020206</td>\n",
       "      <td>30202</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>3111</td>\n",
       "      <td>20230618</td>\n",
       "      <td>18.6840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3020608</td>\n",
       "      <td>30206</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>3278</td>\n",
       "      <td>20240731</td>\n",
       "      <td>396.1008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3020809</td>\n",
       "      <td>30208</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>3603</td>\n",
       "      <td>20230807</td>\n",
       "      <td>6.2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3020608</td>\n",
       "      <td>30206</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>4604</td>\n",
       "      <td>20230131</td>\n",
       "      <td>27.4032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3040808</td>\n",
       "      <td>30408</td>\n",
       "      <td>304</td>\n",
       "      <td>3</td>\n",
       "      <td>3041</td>\n",
       "      <td>20230906</td>\n",
       "      <td>6.2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864589</th>\n",
       "      <td>3020809</td>\n",
       "      <td>30208</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>1694</td>\n",
       "      <td>20230531</td>\n",
       "      <td>100.8936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864590</th>\n",
       "      <td>2020604</td>\n",
       "      <td>20206</td>\n",
       "      <td>202</td>\n",
       "      <td>2</td>\n",
       "      <td>3314</td>\n",
       "      <td>20230308</td>\n",
       "      <td>66.0168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864591</th>\n",
       "      <td>3040601</td>\n",
       "      <td>30406</td>\n",
       "      <td>304</td>\n",
       "      <td>3</td>\n",
       "      <td>2318</td>\n",
       "      <td>20230307</td>\n",
       "      <td>47.3328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864592</th>\n",
       "      <td>3020401</td>\n",
       "      <td>30204</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>4443</td>\n",
       "      <td>20230204</td>\n",
       "      <td>382.3992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864593</th>\n",
       "      <td>1021101</td>\n",
       "      <td>10211</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>2992</td>\n",
       "      <td>20230806</td>\n",
       "      <td>1441.1592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1864594 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         structure_level_4  structure_level_3  structure_level_2  \\\n",
       "0                  3020206              30202                302   \n",
       "1                  3020608              30206                302   \n",
       "2                  3020809              30208                302   \n",
       "3                  3020608              30206                302   \n",
       "4                  3040808              30408                304   \n",
       "...                    ...                ...                ...   \n",
       "1864589            3020809              30208                302   \n",
       "1864590            2020604              20206                202   \n",
       "1864591            3040601              30406                304   \n",
       "1864592            3020401              30204                302   \n",
       "1864593            1021101              10211                102   \n",
       "\n",
       "         structure_level_1   sku  time_key   quantity  \n",
       "0                        3  3111  20230618    18.6840  \n",
       "1                        3  3278  20240731   396.1008  \n",
       "2                        3  3603  20230807     6.2280  \n",
       "3                        3  4604  20230131    27.4032  \n",
       "4                        3  3041  20230906     6.2280  \n",
       "...                    ...   ...       ...        ...  \n",
       "1864589                  3  1694  20230531   100.8936  \n",
       "1864590                  2  3314  20230308    66.0168  \n",
       "1864591                  3  2318  20230307    47.3328  \n",
       "1864592                  3  4443  20230204   382.3992  \n",
       "1864593                  1  2992  20230806  1441.1592  \n",
       "\n",
       "[1864594 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae37378b",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b17cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sales_df, prices_df, campaigns_df):\n",
    "    \"\"\"\n",
    "    Preparar e integrar os três conjuntos de dados.\n",
    "    \"\"\"\n",
    "    print(\"Processando e integrando os dados...\")\n",
    "    \n",
    "    # Converter time_key para formato de data\n",
    "    sales_df['date'] = pd.to_datetime(sales_df['time_key'].astype(str), format='%Y%m%d')\n",
    "    prices_df['date'] = pd.to_datetime(prices_df['time_key'].astype(str), format='%Y%m%d')\n",
    "    \n",
    "    # Converter datas de campanha para formato datetime\n",
    "    campaigns_df['start_date'] = pd.to_datetime(campaigns_df['start_date'])\n",
    "    campaigns_df['end_date'] = pd.to_datetime(campaigns_df['end_date'])\n",
    "    \n",
    "    # Calcular o target_price: pvp_was * (1 - discount)\n",
    "    prices_df['target_price'] = prices_df['pvp_was'] * (1 - prices_df['discount'])\n",
    "    \n",
    "    # Preencher valores nulos em leaflet\n",
    "    prices_df['leaflet'] = prices_df['leaflet'].fillna('none')\n",
    "    \n",
    "    return sales_df, prices_df, campaigns_df\n",
    "\n",
    "def merge_data(sales_df, prices_df, campaigns_df):\n",
    "    \"\"\"\n",
    "    Juntar os dados em um único dataframe para análise e modelagem.\n",
    "    Versão otimizada para maior desempenho.\n",
    "    \"\"\"\n",
    "    print(\"Juntando os dados...\")\n",
    "    \n",
    "    # Selecionar apenas as colunas necessárias para reduzir uso de memória\n",
    "    df = prices_df[['sku', 'date', 'competitor', 'pvp_was', 'discount', 'flag_promo', 'leaflet', 'target_price']].copy()\n",
    "    \n",
    "    # Agregar vendas por sku e data uma única vez\n",
    "    sales_agg = sales_df.groupby(['sku', 'date'])['quantity'].sum().reset_index()\n",
    "    \n",
    "    # Fazer o merge usando um join mais eficiente\n",
    "    df = pd.merge(df, sales_agg, on=['sku', 'date'], how='left')\n",
    "    \n",
    "    # Adicionar informações de estrutura hierárquica do produto\n",
    "    structure_cols = ['structure_level_1', 'structure_level_2', 'structure_level_3', 'structure_level_4']\n",
    "    structure_info = sales_df[['sku'] + structure_cols].drop_duplicates().set_index('sku')\n",
    "    \n",
    "    # Merge usando indices para maior eficiência\n",
    "    df = df.merge(structure_info, left_on='sku', right_index=True, how='left')\n",
    "    \n",
    "    # Adicionar informações de campanhas de forma vetorizada\n",
    "    print(\"Aplicando informações de campanhas (método otimizado)...\")\n",
    "    \n",
    "    # Inicializar coluna padrão\n",
    "    df['active_campaign'] = 'no_campaign'\n",
    "    \n",
    "    # Criar um dataframe expandido com todas as datas possíveis para campanhas\n",
    "    campaign_dates = []\n",
    "    for _, campaign in campaigns_df.iterrows():\n",
    "        date_range = pd.date_range(start=campaign['start_date'], end=campaign['end_date'])\n",
    "        for date in date_range:\n",
    "            campaign_dates.append({\n",
    "                'date': date,\n",
    "                'competitor': campaign['competitor'],\n",
    "                'chain_campaign': campaign['chain_campaign']\n",
    "            })\n",
    "    \n",
    "    if campaign_dates:  # Verificar se a lista não está vazia\n",
    "        campaign_df = pd.DataFrame(campaign_dates)\n",
    "        \n",
    "        # Fazer merge eficiente usando as datas e competidores como chave\n",
    "        df = pd.merge(\n",
    "            df,\n",
    "            campaign_df,\n",
    "            on=['date', 'competitor'],\n",
    "            how='left',\n",
    "            suffixes=('', '_campaign')\n",
    "        )\n",
    "        \n",
    "        # Atualizar a coluna active_campaign onde temos correspondência\n",
    "        mask = ~df['chain_campaign'].isna()\n",
    "        df.loc[mask, 'active_campaign'] = df.loc[mask, 'chain_campaign']\n",
    "        \n",
    "        # Remover a coluna redundante\n",
    "        df = df.drop('chain_campaign', axis=1)\n",
    "    \n",
    "    # Adicionar recursos temporais de forma vetorizada\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Preencher valores nulos na coluna de quantidade com 0\n",
    "    df['quantity'] = df['quantity'].fillna(0)\n",
    "    \n",
    "    # Liberar memória\n",
    "    del sales_agg, structure_info\n",
    "    gc.collect()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Criar recursos adicionais que podem ajudar na previsão de preços.\n",
    "    Versão otimizada para maior desempenho.\n",
    "    \"\"\"\n",
    "    print(\"Criando features adicionais...\")\n",
    "    \n",
    "    # Fazer cálculos estatísticos em um único passo para evitar múltiplos groupby\n",
    "    print(\"Calculando estatísticas...\")\n",
    "    \n",
    "    # Estatísticas por SKU\n",
    "    sku_stats = df.groupby('sku')['target_price'].agg(['mean', 'std', 'min', 'max']).reset_index()\n",
    "    sku_stats.columns = ['sku', 'mean_price', 'std_price', 'min_price', 'max_price']\n",
    "    \n",
    "    # Média de preço por competitor\n",
    "    #comp_price = df.groupby('competitor')['target_price'].mean().reset_index()\n",
    "    #comp_price.columns = ['competitor', 'competitor_avg_price']\n",
    "    \n",
    "    # Médias por níveis de estrutura (mais eficiente)\n",
    "    structure_prices = {}\n",
    "    for level in ['structure_level_1', 'structure_level_2', 'structure_level_3', 'structure_level_4']:\n",
    "        level_price = df.groupby(level)['target_price'].mean().reset_index()\n",
    "        level_price.columns = [level, f'{level}_avg_price']\n",
    "        structure_prices[level] = level_price\n",
    "    \n",
    "    # Fazer merges em sequência é mais eficiente que múltiplos merges independentes\n",
    "    df = pd.merge(df, sku_stats, on='sku', how='left')\n",
    "    #df = pd.merge(df, comp_price, on='competitor', how='left')\n",
    "    \n",
    "    for level in ['structure_level_1', 'structure_level_2', 'structure_level_3', 'structure_level_4']:\n",
    "        df = pd.merge(df, structure_prices[level], on=level, how='left')\n",
    "    \n",
    "    # Marcar períodos promocionais (baseado em flag_promo)\n",
    "    df['is_promo_period'] = df['flag_promo'].astype(int)\n",
    "    \n",
    "    # Criar indicadores para campanhas e leaflets usando get_dummies (mais eficiente)\n",
    "    print(\"Criando indicadores para campanhas e leaflets...\")\n",
    "    campaign_dummies = pd.get_dummies(df['active_campaign'], prefix='campaign')\n",
    "    leaflet_dummies = pd.get_dummies(df['leaflet'], prefix='leaflet')\n",
    "    \n",
    "    # Concatenar de uma vez\n",
    "    df = pd.concat([df, campaign_dummies, leaflet_dummies], axis=1)\n",
    "    \n",
    "    # Liberar memória\n",
    "    #del sku_stats, comp_price, structure_prices, campaign_dummies, leaflet_dummies\n",
    "    del sku_stats, structure_prices, campaign_dummies, leaflet_dummies\n",
    "    gc.collect()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_train_test(df, target_competitor):\n",
    "    \"\"\"\n",
    "    Preparar conjuntos de treinamento e teste.\n",
    "    \"\"\"\n",
    "    print(\"Preparando conjuntos de treinamento e teste...\")\n",
    "    \n",
    "    # Filtrar para competidores alvo\n",
    "    pred_df = df[df['competitor'] == target_competitor].copy()\n",
    "    \n",
    "    # Definir recursos e alvo    \n",
    "    '''\n",
    "    features = [\n",
    "        'sku', 'structure_level_1', 'structure_level_2', 'structure_level_3', 'structure_level_4',\n",
    "        'year', 'month', 'day_of_week', 'is_weekend', 'is_promo_period',\n",
    "        'discount', 'flag_promo', 'quantity', 'competitor_avg_price', 'mean_price', 'std_price', 'min_price', 'max_price',\n",
    "        'structure_level_1_avg_price', 'structure_level_2_avg_price', 'structure_level_3_avg_price', 'structure_level_4_avg_price'        \n",
    "    ]\n",
    "    #'''\n",
    "\n",
    "    features = [\n",
    "        'sku', 'structure_level_1', 'structure_level_2', 'structure_level_3', 'structure_level_4',\n",
    "        'year', 'month', 'day_of_week', 'is_weekend', 'is_promo_period',\n",
    "        'discount', 'flag_promo', 'quantity', 'mean_price', 'std_price', 'min_price', 'max_price',\n",
    "        'structure_level_1_avg_price', 'structure_level_2_avg_price', 'structure_level_3_avg_price', 'structure_level_4_avg_price'        \n",
    "    ]\n",
    "\n",
    "    # Adicionar colunas de campanhas e leaflets\n",
    "    campaign_cols = [col for col in pred_df.columns if col.startswith('campaign_')]\n",
    "    leaflet_cols = [col for col in pred_df.columns if col.startswith('leaflet_')]\n",
    "    features.extend(campaign_cols)\n",
    "    features.extend(leaflet_cols)\n",
    "\n",
    "    # Obter X e y\n",
    "    X = pred_df[features]\n",
    "    y = pred_df['target_price']\n",
    "\n",
    "    return X, y, features, pred_df\n",
    "\n",
    "def build_and_train_model(X, y, df, target_competitor):\n",
    "    \"\"\"\n",
    "    Construir e treinar o modelo de previsão.\n",
    "    \"\"\"\n",
    "    print(\"Construindo e treinando o modelo...\")\n",
    "    \n",
    "\n",
    "    # Extract product category for grouped evaluation\n",
    "    categories = df['structure_level_2']\n",
    "\n",
    "    # --- 1. Configure TimeSeriesSplit ---\n",
    "\n",
    "    # Set up time-based cross-validation\n",
    "    n_samples = len(X)\n",
    "    max_folds = min(6, n_samples - 1)  # Ensure at least 2 samples per fold\n",
    "    if max_folds < 2:\n",
    "        raise ValueError(f\"Not enough data for time-based cross-validation ({n_samples} samples).\")\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=max_folds)\n",
    "    print(f\"TimeSeriesSplit with {max_folds} folds on {n_samples} samples\")\n",
    "\n",
    "    # Containers to store results\n",
    "    mae_scores = []\n",
    "    category_metrics = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):\n",
    "        # Split data into training and test sets\n",
    "        X_train, X_test = X.iloc[train_idx].copy(), X.iloc[test_idx].copy()\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        cat_test = categories.iloc[test_idx]\n",
    "\n",
    "        # Create LightGBM datasets\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "        # --- Model Parameters (choose one version at a time) ---\n",
    "\n",
    "        ### Version 1 - Basic tuned GBDT\n",
    "        #'''\n",
    "        params = {\n",
    "            'objective': 'regression',           # Type of task: 'regression' for continuous output prediction\n",
    "            'metric': 'mae',                     # Metric used to evaluate performance: 'mae' = Mean Absolute Error\n",
    "            'boosting_type': 'gbdt',             # Boosting algorithm: 'gbdt' = Gradient Boosted Decision Trees\n",
    "\n",
    "            'num_leaves': 21,                    # Max number of leaves in one tree (controls model complexity)\n",
    "            'learning_rate': 0.01,               # Shrinks the contribution of each tree (lower = better generalization)\n",
    "            'max_depth': 6,                      # Max depth of trees (limits overfitting; -1 means no limit)\n",
    "\n",
    "            'lambda_l1': 0.1,                    # L1 regularization (controls model complexity)\n",
    "            'lambda_l2': 0.1,                    # L2 regularization (controls model complexity)\n",
    "\n",
    "            'feature_fraction': 0.9,             # Fraction of features used per iteration (column sampling)\n",
    "            'bagging_fraction': 0.8,             # Fraction of data used per iteration (row sampling)\n",
    "            'bagging_freq': 5,                   # Perform bagging every k iterations (0 = disable)\n",
    "\n",
    "            'verbose': -1,                       # Controls logging: < 0 means no output\n",
    "            'seed': 42                           # Random seed for reproducibility\n",
    "        }\n",
    "        #'''\n",
    "\n",
    "\n",
    "\n",
    "        # --- Additional useful parameters you can try ---\n",
    "\n",
    "        # 'min_data_in_leaf': 20,               # Minimum number of samples per leaf (higher reduces overfitting)\n",
    "        # 'min_gain_to_split': 0.01,            # Minimum loss reduction required to make a further partition\n",
    "        # 'lambda_l1': 0.1,                     # L1 regularization (controls model complexity)\n",
    "        # 'lambda_l2': 0.1,                     # L2 regularization (controls model complexity)\n",
    "        # 'max_bin': 255,                       # Max number of bins used for discretizing continuous features\n",
    "        # 'early_stopping_round': 50,          # Early stopping based on validation metric (used via callbacks)\n",
    "        # 'first_metric_only': True,           # Stop training if the first metric does not improve (only use first listed metric)\n",
    "        # 'drop_rate': 0.1,                     # (For DART) Dropout rate for trees\n",
    "        # 'skip_drop': 0.5,                     # (For DART) Probability of skipping dropout\n",
    "        # 'xgboost_dart_mode': False,          # (For DART) Whether to use xgboost's dart mode\n",
    "        # 'extra_trees': False,                # Whether to use extremely randomized trees (more randomness)\n",
    "        # 'force_row_wise': False,             # Forces row-wise histogram building (useful for GPU or speed tuning)\n",
    "        # 'deterministic': True,               # Ensures reproducibility of results (same seed = same result)\n",
    "\n",
    "\n",
    "\n",
    "        ### Version 2 - GBDT with regularization and deeper trees\n",
    "        '''\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mae',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 63,\n",
    "            'learning_rate': 0.01,\n",
    "            'max_depth': 8,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.7,\n",
    "            'bagging_freq': 4,\n",
    "            'min_data_in_leaf': 20,\n",
    "            'lambda_l1': 0.1,\n",
    "            'lambda_l2': 0.1,\n",
    "            'verbose': -1,\n",
    "            'seed': 42,\n",
    "            'first_metric_only': True\n",
    "        }\n",
    "        #'''\n",
    "\n",
    "        ### Version 3 - Using DART boosting\n",
    "        '''\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mae',\n",
    "            'boosting_type': 'dart',        # Dropouts meet Multiple Additive Regression Trees\n",
    "            'num_leaves': 41,\n",
    "            'learning_rate': 0.03,\n",
    "            'max_depth': 6,\n",
    "            'feature_fraction': 0.85,\n",
    "            'bagging_fraction': 0.85,\n",
    "            'bagging_freq': 5,\n",
    "            'min_data_in_leaf': 30,\n",
    "            'lambda_l1': 0.05,\n",
    "            'lambda_l2': 0.15,\n",
    "            'min_gain_to_split': 0.02,\n",
    "            'drop_rate': 0.1,\n",
    "            'xgboost_dart_mode': True,          # (For DART) Whether to use xgboost's dart mode\n",
    "            'max_drop': 50,\n",
    "            'verbose': -1,\n",
    "            'seed': 42\n",
    "        }\n",
    "        #'''\n",
    "\n",
    "        # --- Train Model ---\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            valid_sets=[valid_data],\n",
    "            num_boost_round=1000,\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "                lgb.log_evaluation(period=0)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Save model from last fold\n",
    "        if fold == max_folds:\n",
    "            model_filename = f\"lgbm_{target_competitor}_{datetime.now().strftime('%Y%m%d_%H%M')}.pkl\"\n",
    "            joblib.dump(model, model_filename)\n",
    "            print(f\"\\nModel saved as: {model_filename}\")\n",
    "\n",
    "        # --- Evaluate predictions ---\n",
    "        y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "        fold_mae = mean_absolute_error(y_test, y_pred)\n",
    "        mae_scores.append(fold_mae)\n",
    "        print(f\"Fold {fold} MAE: {fold_mae:.2f}\")\n",
    "\n",
    "        # Create dataframe for category-wise metrics\n",
    "        df_cat = pd.DataFrame({\n",
    "            'category': cat_test,\n",
    "            'y_true': y_test,\n",
    "            'y_pred': y_pred\n",
    "        })\n",
    "\n",
    "        # Function to compute key regression metrics\n",
    "        def get_metrics(df_cat):\n",
    "            y_true = df_cat['y_true']\n",
    "            y_pred = df_cat['y_pred']\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100 if np.all(y_true != 0) else np.nan\n",
    "            return pd.Series({'MAE': mae, 'RMSE': rmse, 'R2': r2, 'MAPE': mape})\n",
    "\n",
    "        # Store category-level metrics\n",
    "        cat_metrics = df_cat.groupby('category').apply(get_metrics)\n",
    "        category_metrics.append(cat_metrics)\n",
    "    \n",
    "    return model, category_metrics, mae_scores\n",
    "\n",
    "def evaluate_model(category_metrics, df):\n",
    "    \"\"\"\n",
    "    Avaliar o desempenho do modelo.\n",
    "    \"\"\"\n",
    "    print(\"Avaliando o modelo...\")\n",
    "    \n",
    "    # Combine metrics from all folds into a single dataframe\n",
    "    df_all_metrics = pd.concat(category_metrics, keys=range(1, len(category_metrics) + 1), names=['Fold', 'Category'])\n",
    "    mean_metrics_per_cat = df_all_metrics.groupby('Category').mean()\n",
    "\n",
    "    # --- 3. Final Summary ---\n",
    "\n",
    "    # Global MAE statistics\n",
    "    mae_scores = [cat['MAE'].mean() for cat in category_metrics]\n",
    "    print(f\"\\nAverage MAE across folds: {np.mean(mae_scores):.2f}, Std MAE: {np.std(mae_scores):.2f}\")\n",
    "\n",
    "    # Category-wise MAE statistics with stability check\n",
    "    category_mae_df = pd.DataFrame({f'Fold_{i+1}': cat['MAE'] for i, cat in enumerate(category_metrics)})\n",
    "    category_mae_df['Mean_MAE'] = category_mae_df.mean(axis=1)\n",
    "    category_mae_df['Std_MAE'] = category_mae_df.std(axis=1)\n",
    "    category_mae_df['Stability_Ratio'] = category_mae_df['Std_MAE'] / category_mae_df['Mean_MAE']\n",
    "\n",
    "    print(\"\\nCategory-wise MAE performance:\")\n",
    "    print(category_mae_df[['Mean_MAE', 'Std_MAE', 'Stability_Ratio']].sort_values('Mean_MAE'))\n",
    "\n",
    "    # Stability threshold check\n",
    "    max_ratio = category_mae_df['Stability_Ratio'].max()\n",
    "    print(f\"\\nMax Stability Ratio: {max_ratio:.2%}\")\n",
    "    if max_ratio < 0.10:\n",
    "        print(\"SUCCESS: Variation across folds is under 10%\")\n",
    "    else:\n",
    "        print(\"WARNING: Some categories exceed 10% variation across folds\")\n",
    "\n",
    "    # --- 4. HTML Summary Table ---\n",
    "\n",
    "    # Statistics for the actual target prices by category\n",
    "    target_stats = df.groupby('structure_level_2')['target_price'].agg(['mean', 'median', 'std'])\n",
    "    target_stats.columns = ['Mean_Target_Price', 'Median_Target_Price', 'Std_Target_Price']\n",
    "\n",
    "    # Merge prediction errors with target stats\n",
    "    category_summary = category_mae_df[['Mean_MAE', 'Std_MAE', 'Stability_Ratio']].join(target_stats)\n",
    "\n",
    "    # Relative error comparison\n",
    "    category_summary['MAE_%_of_Mean'] = category_summary['Mean_MAE'] / category_summary['Mean_Target_Price']\n",
    "    category_summary['MAE_%_of_Median'] = category_summary['Mean_MAE'] / category_summary['Median_Target_Price']\n",
    "\n",
    "    # Sort by relative MAE\n",
    "    category_summary_sorted = category_summary.sort_values('MAE_%_of_Mean')\n",
    "\n",
    "    # Display in notebook (HTML)\n",
    "    from IPython.display import display, HTML\n",
    "    print(\"\\nCategory-level model performance compared to target_price statistics:\")\n",
    "    display(HTML(category_summary_sorted[['Mean_MAE', 'Std_MAE',\n",
    "                                        'Mean_Target_Price', 'Median_Target_Price',\n",
    "                                        'MAE_%_of_Mean', 'MAE_%_of_Median',\n",
    "                                        'Stability_Ratio']].to_html(float_format=\"%.2f\")))\n",
    "\n",
    "def predict_price(sku, date, competitor, sales_df_clean, prices_df_clean, campaigns_df_clean, features, model):\n",
    "    \"\"\"\n",
    "    Fazer uma previsão para um novo caso.\n",
    "    \"\"\"\n",
    "    if sku not in sales_df_clean['sku'].unique():\n",
    "        return\n",
    "\n",
    "    print(\"Create missing data to make predictions...\")\n",
    "    \n",
    "    # Extrair informações estruturais do SKU\n",
    "    sku_info = sales_df_clean[sales_df_clean['sku'] == sku][['structure_level_1', 'structure_level_2', \n",
    "                                            'structure_level_3', 'structure_level_4']].iloc[0]\n",
    "    \n",
    "    # Converter date para datetime se for string\n",
    "    if isinstance(date, str):\n",
    "        date = pd.to_datetime(date)\n",
    "        \n",
    "    # Criar dataframe para este caso específico com valores padrão\n",
    "    new_data = {\n",
    "        'sku': sku,\n",
    "        'date': date,\n",
    "        'competitor': competitor,\n",
    "        'structure_level_1': sku_info['structure_level_1'],\n",
    "        'structure_level_2': sku_info['structure_level_2'],\n",
    "        'structure_level_3': sku_info['structure_level_3'],\n",
    "        'structure_level_4': sku_info['structure_level_4'],\n",
    "        'year': date.year,\n",
    "        'month': date.month,\n",
    "        'day_of_week': date.dayofweek,\n",
    "        'is_weekend': 1 if date.dayofweek in [5, 6] else 0,\n",
    "        'is_promo_period': 0,  # Valor padrão\n",
    "    }\n",
    "    \n",
    "    print(\"Check campains.\")\n",
    "    # Verificar campanhas ativas na data\n",
    "    active_campaign = 'no_campaign'\n",
    "    for _, campaign in campaigns_df_clean[campaigns_df_clean['competitor'] == competitor].iterrows():\n",
    "        if campaign['start_date'] <= date <= campaign['end_date']:\n",
    "            active_campaign = campaign['chain_campaign']\n",
    "            break\n",
    "    \n",
    "    print(\"Check active_campaign.\")\n",
    "    # Preencher valores para campanhas\n",
    "    for feature in features:\n",
    "        if feature.startswith('campaign_'):\n",
    "            campaign_name = feature.replace('campaign_', '')\n",
    "            new_data[feature] = 1 if active_campaign == campaign_name or feature == '' else 0\n",
    "    \n",
    "    print(\"Check leaflets.\")\n",
    "    # Preencher valores para leaflets\n",
    "    for feature in features:\n",
    "        if feature.startswith('leaflet_'):\n",
    "            leaflet_name = feature.replace('leaflet_', '')\n",
    "            new_data[feature] = 1 if leaflet_name == 'none' else 0\n",
    "\n",
    "    print(f'Check mean, std, min and max prices for sku.')\n",
    "    # Para valores estatísticos, usar médias do dataframe completo\n",
    "    # Esta é uma simplificação; em produção, você armazenaria estas estatísticas\n",
    "    prices_stats = prices_df_clean[prices_df_clean['sku'] == sku]['target_price']\n",
    "    if len(prices_stats) > 0:\n",
    "        new_data['mean_price'] = prices_stats.mean()\n",
    "        new_data['std_price'] = prices_stats.std() if len(prices_stats) > 1 else 0\n",
    "        new_data['min_price'] = prices_stats.min()\n",
    "        new_data['max_price'] = prices_stats.max()\n",
    "\n",
    "    # Médias por nível estrutural\n",
    "    for level in ['structure_level_1', 'structure_level_2', 'structure_level_3', 'structure_level_4']:\n",
    "        # Obter os valores únicos do nível atual para o SKU específico\n",
    "        list_structure_values = sales_df_clean[sales_df_clean['sku'] == sku][level].unique()\n",
    "        \n",
    "        # Filtrar preços para SKUs que têm o mesmo valor de estrutura\n",
    "        filtered_prices = prices_df_clean[prices_df_clean['sku'].isin(list_structure_values)]\n",
    "        \n",
    "        # Calcular o preço médio\n",
    "        avg_price = filtered_prices['target_price'].mean()\n",
    "        \n",
    "        # Atribuir o preço médio à coluna correspondente\n",
    "        new_data[f'{level}_avg_price'] = avg_price\n",
    "  \n",
    "    print('Check competitor_avg_price.')\n",
    "    # Média de preço do competidor\n",
    "    '''\n",
    "    comp_prices = prices_df[prices_df['competitor'] == competitor]['target_price']\n",
    "    new_data['competitor_avg_price'] = comp_prices.mean() #if len(comp_prices) > 0 else new_data['mean_price']\n",
    "    #'''\n",
    "\n",
    "    # Criar dataframe\n",
    "    new_case = pd.DataFrame([new_data])\n",
    "    \n",
    "    # Selecionar apenas as features usadas no modelo\n",
    "    available_features = [f for f in features if f in new_case.columns]\n",
    "    X_pred = new_case[available_features]\n",
    "    \n",
    "    # Verificar quais features estão faltando e adicionar valores padrão\n",
    "    missing_features = set(features) - set(available_features)\n",
    "    for feature in missing_features:\n",
    "        if feature.startswith('campaign_'):\n",
    "            X_pred[feature] = 0\n",
    "        elif feature.startswith('leaflet_'):\n",
    "            X_pred[feature] = 0 if feature != 'leaflet_none' else 1\n",
    "        else:\n",
    "            # Para outras features, usar a média global\n",
    "            X_pred[feature] = 0\n",
    "    \n",
    "    print(\"Predicting target_price.\")\n",
    "    \n",
    "    # Fazer a previsão\n",
    "    predicted_price = model.predict(X_pred)[0]\n",
    "    \n",
    "    return predicted_price\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ccdaf",
   "metadata": {},
   "source": [
    "# Prepare and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0ad59c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando e integrando os dados...\n",
      "Juntando os dados...\n",
      "Aplicando informações de campanhas (método otimizado)...\n",
      "Criando features adicionais...\n",
      "Calculando estatísticas...\n",
      "Criando indicadores para campanhas e leaflets...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sku                                     int64\n",
       "date                           datetime64[ns]\n",
       "competitor                             object\n",
       "pvp_was                               float64\n",
       "discount                              float64\n",
       "flag_promo                              int64\n",
       "leaflet                                object\n",
       "target_price                          float64\n",
       "quantity                              float64\n",
       "structure_level_1                     float64\n",
       "structure_level_2                     float64\n",
       "structure_level_3                     float64\n",
       "structure_level_4                     float64\n",
       "active_campaign                        object\n",
       "year                                    int32\n",
       "month                                   int32\n",
       "day_of_week                             int32\n",
       "is_weekend                              int32\n",
       "mean_price                            float64\n",
       "std_price                             float64\n",
       "min_price                             float64\n",
       "max_price                             float64\n",
       "structure_level_1_avg_price           float64\n",
       "structure_level_2_avg_price           float64\n",
       "structure_level_3_avg_price           float64\n",
       "structure_level_4_avg_price           float64\n",
       "is_promo_period                         int32\n",
       "campaign_A1                              bool\n",
       "campaign_A2                              bool\n",
       "campaign_A3                              bool\n",
       "campaign_C1                              bool\n",
       "campaign_C2                              bool\n",
       "campaign_no_campaign                     bool\n",
       "leaflet_none                             bool\n",
       "leaflet_short                            bool\n",
       "leaflet_themed                           bool\n",
       "leaflet_weekly                           bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparar e integrar os dados\n",
    "#sales_df, prices_df, campaigns_df = prepare_data(sales_df, prices_df, campaigns_df)\n",
    "sales_df_clean, prices_df_clean, campaigns_df_clean = prepare_data(sales_df, prices_df, campaigns_df)\n",
    "\n",
    "sales_df_clean.to_parquet(\"sales_df_clean.parquet\")\n",
    "prices_df_clean.to_parquet(\"prices_df_clean.parquet\")\n",
    "campaigns_df_clean.to_parquet(\"campaigns_df_clean.parquet\")\n",
    "\n",
    "\n",
    "#df = merge_data(sales_df, prices_df, campaigns_df)\n",
    "df = merge_data(sales_df_clean, prices_df_clean, campaigns_df_clean)\n",
    "df = feature_engineering(df)\n",
    "\n",
    "df.to_parquet(\"df.parquet\")\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f74509",
   "metadata": {},
   "source": [
    "# Competitor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dadd1434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntarget_competitor = 'competitorA'\\n\\nX, y, features, df_model = prepare_train_test(df,target_competitor)\\nmodel, category_metrics, mae_scores = build_and_train_model(X, y, df_model, target_competitor)\\n#\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "target_competitor = 'competitorA'\n",
    "\n",
    "X, y, features, df_model = prepare_train_test(df,target_competitor)\n",
    "model, category_metrics, mae_scores = build_and_train_model(X, y, df_model, target_competitor)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1203d50b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ea5e979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel, category_metrics, mae_scores = build_and_train_model(X, y, df_model, target_competitor)\\nevaluate_model(category_metrics, df_model)\\n#'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model, category_metrics, mae_scores = build_and_train_model(X, y, df_model, target_competitor)\n",
    "evaluate_model(category_metrics, df_model)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118dba8f",
   "metadata": {},
   "source": [
    "# Pipeline Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894eda9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando conjuntos de treinamento e teste...\n",
      "Construindo e treinando o modelo...\n",
      "TimeSeriesSplit with 6 folds on 1099604 samples\n",
      "Fold 1 MAE: 2.22\n",
      "Fold 2 MAE: 2.22\n",
      "Fold 3 MAE: 2.19\n",
      "Fold 4 MAE: 2.19\n",
      "Fold 5 MAE: 2.19\n",
      "\n",
      "Model saved as: lgbm_competitorA_20250520_2004.pkl\n",
      "Fold 6 MAE: 2.20\n",
      "Preparando conjuntos de treinamento e teste...\n",
      "Construindo e treinando o modelo...\n",
      "TimeSeriesSplit with 6 folds on 268763 samples\n",
      "Fold 1 MAE: 2.21\n",
      "Fold 2 MAE: 2.14\n",
      "Fold 3 MAE: 2.16\n",
      "Fold 4 MAE: 2.15\n",
      "Fold 5 MAE: 2.17\n",
      "\n",
      "Model saved as: lgbm_competitorB_20250520_2005.pkl\n",
      "Fold 6 MAE: 2.10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models_registry.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "df = pd.read_parquet(\"df.parquet\")\n",
    "\n",
    "model_registry = {}\n",
    "\n",
    "for comp in df['competitor'].unique():\n",
    "    if comp == 'chain':\n",
    "        continue\n",
    "    \n",
    "    X, y, features, df_model = prepare_train_test(df, comp)\n",
    "    model, category_metrics, mae_scores = build_and_train_model(X, y, df_model, comp)\n",
    "\n",
    "    model_registry[comp] = {\n",
    "        #\"X\": X,\n",
    "        #\"y\": y,\n",
    "        \"features\": features,\n",
    "        \"df_model\": df_model,\n",
    "        \"model\": model,\n",
    "        \"columns\": list(X.columns),\n",
    "        \"dtypes\": X.dtypes.to_dict(),  # guarda os tipos para validação na API        \n",
    "        \"metrics\": category_metrics,\n",
    "        \"mae\": mae_scores\n",
    "    }\n",
    "\n",
    "# Salvar tudo num arquivo\n",
    "joblib.dump(model_registry, \"models_registry.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e0870",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aceffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df_clean = pd.read_parquet(\"sales_df_clean.parquet\")\n",
    "prices_df_clean  = pd.read_parquet(\"prices_df_clean.parquet\")\n",
    "campaigns_df_clean  = pd.read_parquet(\"campaigns_df_clean.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0343709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>structure_level_4</th>\n",
       "      <th>structure_level_3</th>\n",
       "      <th>structure_level_2</th>\n",
       "      <th>structure_level_1</th>\n",
       "      <th>sku</th>\n",
       "      <th>time_key</th>\n",
       "      <th>quantity</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3020206</td>\n",
       "      <td>30202</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>3111</td>\n",
       "      <td>20230618</td>\n",
       "      <td>18.6840</td>\n",
       "      <td>2023-06-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3020608</td>\n",
       "      <td>30206</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>3278</td>\n",
       "      <td>20240731</td>\n",
       "      <td>396.1008</td>\n",
       "      <td>2024-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3020809</td>\n",
       "      <td>30208</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>3603</td>\n",
       "      <td>20230807</td>\n",
       "      <td>6.2280</td>\n",
       "      <td>2023-08-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3020608</td>\n",
       "      <td>30206</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>4604</td>\n",
       "      <td>20230131</td>\n",
       "      <td>27.4032</td>\n",
       "      <td>2023-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3040808</td>\n",
       "      <td>30408</td>\n",
       "      <td>304</td>\n",
       "      <td>3</td>\n",
       "      <td>3041</td>\n",
       "      <td>20230906</td>\n",
       "      <td>6.2280</td>\n",
       "      <td>2023-09-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864589</th>\n",
       "      <td>3020809</td>\n",
       "      <td>30208</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>1694</td>\n",
       "      <td>20230531</td>\n",
       "      <td>100.8936</td>\n",
       "      <td>2023-05-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864590</th>\n",
       "      <td>2020604</td>\n",
       "      <td>20206</td>\n",
       "      <td>202</td>\n",
       "      <td>2</td>\n",
       "      <td>3314</td>\n",
       "      <td>20230308</td>\n",
       "      <td>66.0168</td>\n",
       "      <td>2023-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864591</th>\n",
       "      <td>3040601</td>\n",
       "      <td>30406</td>\n",
       "      <td>304</td>\n",
       "      <td>3</td>\n",
       "      <td>2318</td>\n",
       "      <td>20230307</td>\n",
       "      <td>47.3328</td>\n",
       "      <td>2023-03-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864592</th>\n",
       "      <td>3020401</td>\n",
       "      <td>30204</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>4443</td>\n",
       "      <td>20230204</td>\n",
       "      <td>382.3992</td>\n",
       "      <td>2023-02-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864593</th>\n",
       "      <td>1021101</td>\n",
       "      <td>10211</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>2992</td>\n",
       "      <td>20230806</td>\n",
       "      <td>1441.1592</td>\n",
       "      <td>2023-08-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1864594 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         structure_level_4  structure_level_3  structure_level_2  \\\n",
       "0                  3020206              30202                302   \n",
       "1                  3020608              30206                302   \n",
       "2                  3020809              30208                302   \n",
       "3                  3020608              30206                302   \n",
       "4                  3040808              30408                304   \n",
       "...                    ...                ...                ...   \n",
       "1864589            3020809              30208                302   \n",
       "1864590            2020604              20206                202   \n",
       "1864591            3040601              30406                304   \n",
       "1864592            3020401              30204                302   \n",
       "1864593            1021101              10211                102   \n",
       "\n",
       "         structure_level_1   sku  time_key   quantity       date  \n",
       "0                        3  3111  20230618    18.6840 2023-06-18  \n",
       "1                        3  3278  20240731   396.1008 2024-07-31  \n",
       "2                        3  3603  20230807     6.2280 2023-08-07  \n",
       "3                        3  4604  20230131    27.4032 2023-01-31  \n",
       "4                        3  3041  20230906     6.2280 2023-09-06  \n",
       "...                    ...   ...       ...        ...        ...  \n",
       "1864589                  3  1694  20230531   100.8936 2023-05-31  \n",
       "1864590                  2  3314  20230308    66.0168 2023-03-08  \n",
       "1864591                  3  2318  20230307    47.3328 2023-03-07  \n",
       "1864592                  3  4443  20230204   382.3992 2023-02-04  \n",
       "1864593                  1  2992  20230806  1441.1592 2023-08-06  \n",
       "\n",
       "[1864594 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df798cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "[3111 3278 3603 4604 3041]\n"
     ]
    }
   ],
   "source": [
    "print(sales_df_clean['sku'].dtype)\n",
    "print(sales_df_clean['sku'].unique()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f34719f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3111 3278 3603 ... 3543 2593 3663]\n"
     ]
    }
   ],
   "source": [
    "print(sales_df_clean['sku'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1340aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create missing data to make predictions...\n",
      "Check campains.\n",
      "Check active_campaign.\n",
      "Check leaflets.\n",
      "Check mean, std, min and max prices for sku.\n",
      "Check competitor_avg_price.\n",
      "Predicting target_price.\n",
      "Preço previsto para SKU 3111, data 2025-06-30 00:00:00, competidor competitorA: 48.23\n"
     ]
    }
   ],
   "source": [
    "# Carregar os dados e modelos\n",
    "model_registry = joblib.load(\"models_registry.pkl\")\n",
    "\n",
    "sales_df_clean = pd.read_parquet(\"sales_df_clean.parquet\")\n",
    "prices_df_clean  = pd.read_parquet(\"prices_df_clean.parquet\")\n",
    "campaigns_df_clean  = pd.read_parquet(\"campaigns_df_clean.parquet\")\n",
    "\n",
    "# Fazer previsões para novos casos\"\n",
    "sku = 3111\n",
    "date = pd.to_datetime('2025-06-30')\n",
    "competitor = 'competitorA'\n",
    "\n",
    "if competitor == 'competitorA':\n",
    "    model = model_registry[competitor][\"model\"]\n",
    "    features = model_registry[competitor][\"features\"]\n",
    "    \n",
    "    predicted_price = predict_price(sku, date, competitor, sales_df_clean, prices_df_clean, campaigns_df_clean, features, model)\n",
    "\n",
    "elif competitor == 'competitorB':\n",
    "    model = model_registry[competitor][\"model\"]\n",
    "    features = model_registry[competitor][\"features\"]\n",
    "    \n",
    "    predicted_price = predict_price(sku, date, competitor, sales_df_clean, prices_df_clean, campaigns_df_clean, features, model)\n",
    "\n",
    "print(f\"Preço previsto para SKU {sku}, data {date}, competidor {competitor}: {predicted_price:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
